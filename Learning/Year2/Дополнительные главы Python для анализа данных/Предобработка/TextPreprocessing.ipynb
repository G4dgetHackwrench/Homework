{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXTFLI4NF-LR"
   },
   "source": [
    "## Этапы (простой) обработки текста\n",
    "\n",
    "<img src=\"images/textm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgf1SvTRF-LS"
   },
   "source": [
    "\n",
    "## Декодирование\n",
    "\n",
    "\n",
    "**Def.**  \n",
    "перевод последовательности байт в последовательность символов\n",
    "\n",
    "* Распаковка  \n",
    "*plain/.zip/.gz/...*\n",
    "* Кодировка  \n",
    "*ASCII/utf-8/Windows-1251/...*\n",
    "* Формат  \n",
    "*csv/xml/json/doc...*\n",
    "\n",
    "Кроме того: что такое документ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3xU0-PjF-LS"
   },
   "source": [
    "## Разбиение на токены\n",
    "**Def.**  \n",
    "разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n",
    "Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n",
    "\n",
    "\n",
    "*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n",
    "\n",
    "\n",
    "**Проблемы:**  \n",
    "* example@example.com, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "Альтернатива: n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1719,
     "status": "ok",
     "timestamp": 1631048074715,
     "user": {
      "displayName": "Михаил Баранов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVV46Gpmty8si64xPQTsh_aZMsXJnF8ADGg9xOTg=s64",
      "userId": "05195714544608108975"
     },
     "user_tz": -180
    },
    "id": "BTJ5nrYYF-LT",
    "outputId": "7d3bd7ab-d9ec-4f33-98df-2d1a4688b602"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anzel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1631048074720,
     "user": {
      "displayName": "Михаил Баранов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjVV46Gpmty8si64xPQTsh_aZMsXJnF8ADGg9xOTg=s64",
      "userId": "05195714544608108975"
     },
     "user_tz": -180
    },
    "id": "Olg2Ah-SF-LV",
    "outputId": "33cd3cf8-dfdf-4f0f-822b-a322096f35b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Трисия\n",
      "любила\n",
      "Нью\n",
      "-\n",
      "Йорк\n",
      ",\n",
      "поскольку\n",
      "любовь\n",
      "к\n",
      "Нью\n",
      "-\n",
      "Йорку\n",
      "могла\n",
      "положительно\n",
      "повлиять\n",
      "на\n",
      "ее\n",
      "карьеру\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\anzel\\AppData\\Local\\Temp\\ipykernel_18448\\2708366142.py:6: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokenizer = RegexpTokenizer(\"\\w+|[^\\w\\s]+\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "s = \"Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"\\w+|[^\\w\\s]+\")\n",
    "for t in tokenizer.tokenize(s): \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buEIu-_CF-LV"
   },
   "source": [
    "## Стоп-слова\n",
    "**Def.**  \n",
    "Наиболее частые слова в языке, не содержащие никакой информации о содержании текста\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uZL-XwB9F-LW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в во не что он на я с со как а то все она так его но да ты\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "print(\" \".join(stopwords.words(\"russian\")[1:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXzzCadWF-LW"
   },
   "source": [
    "Проблема: “To be or not to be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "трисия любила нью-йорк, поскольку любовь нью-йорку могла положительно повлиять карьеру.\n"
     ]
    }
   ],
   "source": [
    "exclude = set(stopwords.words('russian'))\n",
    "print(' '.join(i for i in s.lower().split() if i not in exclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2vMM3qnF-LW"
   },
   "source": [
    "## Нормализация\n",
    "**Def.**  \n",
    "Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n",
    "\n",
    "Подходы  \n",
    "* сформулировать набор правил, по которым преобразуется токен  \n",
    "Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n",
    "* явно хранить связи между токенами (WordNet – Princeton)  \n",
    "машина → автомобиль, Windows 6→ window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "INpfX6wqF-LX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нью-йорк\n"
     ]
    }
   ],
   "source": [
    "s = \"Нью-Йорк\"\n",
    "s1 = s.lower()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wa4NDjrIF-LX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюйорк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s2 = re.sub(r\"\\W\", \"\", s1, flags=re.U)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ESECQgj2F-LY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ньюиорк\n"
     ]
    }
   ],
   "source": [
    "s3 = re.sub(r\"й\", u\"и\", s2, flags=re.U)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxgUV4UbF-LY"
   },
   "source": [
    "## Стемминг и Лемматизация\n",
    "**Def.**  \n",
    "Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n",
    "* Stemming – с помощью простых эвристических правил\n",
    "  * Porter (Cambridge – 1980)\n",
    "        5 этапов, на каждом применяется набор правил, таких как\n",
    "            sses → ss (caresses → caress)\n",
    "            ies → i (ponies → poni)\n",
    "\n",
    "  * Lovins (1968)\n",
    "  * Paice (1990)\n",
    "  * другие\n",
    "* Lemmatization – с использованием словарей и морфологического анализа\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgDqrYkdF-LZ"
   },
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fgpAl1MJF-LZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "stem\n",
      "авиац\n",
      "национальн\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "\n",
    "s = PorterStemmer()\n",
    "print(s.stem(\"Tokenization\"))\n",
    "print(s.stem(\"stemming\"))\n",
    "\n",
    "r = RussianStemmer()\n",
    "print(r.stem(\"Авиация\"))\n",
    "print(r.stem(\"национальный\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfUJXVE4F-LZ"
   },
   "source": [
    "**Наблюдение**  \n",
    "для сложных языков лучше подходит лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nwjaX0zF-La"
   },
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq8bFg_DF-La"
   },
   "outputs": [],
   "source": [
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qSekO34BF-La"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "думать\n"
     ]
    }
   ],
   "source": [
    "import pymorphy3\n",
    "\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "print(morph.parse(\"думающему\")[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc6Z0UY9F-La"
   },
   "source": [
    "## Heaps' law\n",
    "Эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины.\n",
    "\n",
    "$$\n",
    "M = k T^\\beta, \\;M \\text{ -- размер словаря}, \\; T \\text{ -- количество слов в корпусе}\n",
    "$$\n",
    "$$\n",
    "30 \\leq k \\leq 100, \\; b \\approx 0.5\n",
    "$$\n",
    "\n",
    "<img src=\"images/dim.png\">\n",
    "<img src=\"images/heaps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzDJlw5OF-Lb"
   },
   "source": [
    "## Представление документов\n",
    "**Boolean Model.** Присутствие или отсутствие слова в документе  \n",
    "**Bag of Words.** Порядок токенов не важен  \n",
    "\n",
    "*Погода была ужасная, принцесса была прекрасная.\n",
    "Или все было наоборот?*\n",
    "\n",
    "Координаты\n",
    "* Мультиномиальные: количество токенов в документе\n",
    "* Числовые: взвешенное количество токенов в документе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TlHOiiCF-Lb"
   },
   "source": [
    "## Zipf's law\n",
    "Эмпирическая закономерность распределения частоты слов естественного языка\n",
    "\n",
    "$t_1, \\ldots, t_N$ - токены, отранжированные по убыванию частоты\n",
    "   \t\n",
    "$f_1, \\dots, f_N$ - соответствующие частоты\n",
    "\n",
    "**Закон Ципфа**\n",
    "\t$$\n",
    "\tf_i = \\frac{c}{i^k}\n",
    "\t$$\t\n",
    "\t\n",
    "\tЧто еще? Посещаемость сайтов, количество друзей, население городов...\n",
    "<img src=\"images/zipf.png\">\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TextPreprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
